{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4327,
     "status": "ok",
     "timestamp": 1638579693396,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "wWWbqhWb9aNu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict,KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "import scipy\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16290,
     "status": "ok",
     "timestamp": 1638579709683,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "_or4IKY59ccJ",
    "outputId": "9d352677-b999-4225-8db1-8d100009d48b"
   },
   "outputs": [],
   "source": [
    "'''from google.colab import drive\n",
    "drive.mount('/content/drive')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1638579714557,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "GNwCwqy89eP4",
    "outputId": "34fae9fe-e437-4604-9541-f0cd7b77c886"
   },
   "outputs": [],
   "source": [
    "'''import os            ##  This module is for \"operating system\" interfaces\n",
    "import sys           ##  This module is for functionality relevant to the python run time\n",
    "\n",
    "GOOGLE_PATH_AFTER_MYDRIVE = 'NLP_Textcat/spooky_data/train'\n",
    "GOOGLE_DRIVE_PATH = os.path.join('drive','My Drive', GOOGLE_PATH_AFTER_MYDRIVE)\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "\n",
    "# Append the directory path of this notebook to what python easily \"sees\"\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "\n",
    "# Make your current working direct\n",
    "GOOGLE_DRIVE_PATH'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os            ##  This module is for \"operating system\" interfaces\n",
    "import sys           ##  This module is for functionality relevant to the python run time\n",
    "path_to_datafolder = 'C:/Users/mjdom/source/repos/mdst_nlp_2021/data'\n",
    "print(os.listdir(path_to_datafolder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1168,
     "status": "ok",
     "timestamp": 1638579726324,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "ytdo4-Y-9jdI",
    "outputId": "8fb77683-1f64-4957-963d-d1effd8b119a"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(path_to_datafolder+ '/train.csv')\n",
    "df_kaggle = pd.read_csv(path_to_datafolder + '/test.csv')\n",
    "df_kaggle.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1638579733321,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "7ADnsSmT9mLQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "X = df[\"text\"].copy()\n",
    "#X = df[\"text\"]\n",
    "\n",
    "authors = df[\"author\"].copy()\n",
    "\n",
    "# Label data\n",
    "y = []\n",
    "for author in authors:\n",
    "    if author == \"EAP\":\n",
    "        y.append([1, 0, 0])\n",
    "    if author == \"HPL\":\n",
    "        y.append([0, 1, 0])\n",
    "    if author == \"MWS\":\n",
    "        y.append([0, 0, 1])\n",
    "\n",
    "y = np.array(y)\n",
    "\n",
    "y_one_vector = []\n",
    "for author in authors:\n",
    "    if author == \"EAP\":\n",
    "        y_one_vector.append(0)\n",
    "    if author == \"HPL\":\n",
    "        y_one_vector.append(1)\n",
    "    if author == \"MWS\":\n",
    "        y_one_vector.append(2)\n",
    "\n",
    "y_one_vector = np.array(y_one_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13422,
     "status": "ok",
     "timestamp": 1638579767345,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "kewfJfZn9oIg",
    "outputId": "59e5c26f-de83-4f4b-bc12-7fa4f8b40dea"
   },
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.TextVectorization()\n",
    "encoder.adapt(np.hstack((X,df_kaggle['text'])))\n",
    "\n",
    "max_features = 1000000\n",
    "Vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_features, output_mode='tf_idf', ngrams=3)\n",
    "count_vec = tf.keras.layers.TextVectorization(max_tokens=max_features, output_mode='count', sparse=True, ngrams=1)\n",
    "\n",
    "with tf.device('/device:CPU:0'):\n",
    "    Vectorizer.adapt(np.hstack((X,df_kaggle['text'])))\n",
    "    count_vec.adapt(np.hstack((X,df_kaggle['text'])))\n",
    "\n",
    "vocab = encoder.get_vocabulary()\n",
    "len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 111,
     "status": "ok",
     "timestamp": 1638579813070,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "D1NoPyJn9tM4"
   },
   "outputs": [],
   "source": [
    "class CNN1d(tf.keras.Model):\n",
    "    def __init__(self, conv1_filters, conv1_size, conv2_filters, conv2_size, encoder):\n",
    "        super(CNN1d, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        vocab = encoder.get_vocabulary()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=len(vocab),output_dim=128,mask_zero=True)\n",
    "        \n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv1D(filters=conv1_filters,\n",
    "                            kernel_size=conv1_size,\n",
    "                            padding=\"same\",\n",
    "                            activation=\"relu\",\n",
    "                            data_format=\"channels_last\",\n",
    "                            )\n",
    "        self.conv2 = tf.keras.layers.Conv1D(filters=conv2_filters,\n",
    "                            kernel_size=conv2_size,\n",
    "                            padding=\"same\",\n",
    "                            activation=\"relu\",\n",
    "                            data_format=\"channels_last\",\n",
    "                            )\n",
    "        self.global_pool = tf.keras.layers.GlobalMaxPool1D(keepdims=False)\n",
    "        #self.dense1 = tf.keras.layers.Dense(dense1, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(3, activation=\"softmax\")\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        emb = self.encoder(x)\n",
    "        emb = self.embedding(emb)\n",
    "        conv1 = self.conv1(emb)\n",
    "        conv2 = self.conv2(emb)\n",
    "        z = tf.concat([conv1, conv2], axis=2)\n",
    "        z = self.global_pool(z)\n",
    "        #z = self.dense1(z)\n",
    "        z = self.dense2(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1638579821381,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "lgKUrcIv97nJ"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim, mask_zero=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1638580770109,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "gOfJA2C999pQ"
   },
   "outputs": [],
   "source": [
    "def create_model(conv1_filters, conv1_size, conv2_filters, conv2_size):\n",
    "    model = CNN1d(conv1_filters, conv1_size, conv2_filters, conv2_size, encoder)\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_ngram():\n",
    "    model_ngram = tf.keras.Sequential()\n",
    "    model_ngram.add(Vectorizer)\n",
    "      \n",
    "    model_ngram.add(tf.keras.layers.Dense(128, activation='sigmoid'))\n",
    "    model_ngram.add(tf.keras.layers.Dropout(0.5))\n",
    "      \n",
    "    model_ngram.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "      \n",
    "    model_ngram.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                metrics=['accuracy'])\n",
    "    return model_ngram\n",
    "\n",
    "def create_lstm():\n",
    "    LSTM = tf.keras.Sequential()\n",
    "    LSTM.add(encoder)\n",
    "    LSTM.add(tf.keras.layers.Embedding(input_dim=len(vocab),output_dim=256,mask_zero=True))\n",
    "      \n",
    "    LSTM.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,dropout=0.5,return_sequences=True)))\n",
    "    LSTM.add(tf.keras.layers.GlobalMaxPool1D())\n",
    "\n",
    "    LSTM.add(tf.keras.layers.Dropout(0.1))\n",
    "      \n",
    "    LSTM.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "      \n",
    "    LSTM.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return LSTM\n",
    "\n",
    "def create_ensemble():\n",
    "    ensemble = tf.keras.Sequential()\n",
    "    # for 3 model\n",
    "    ensemble.add(tf.keras.layers.Dense(36, activation='sigmoid'))\n",
    "    ensemble.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    ensemble.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "    #ensemble.add(tf.keras.layers.InputLayer())\n",
    "\n",
    "    ensemble.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "def convert_sparce(sparse_tensor):\n",
    "  \n",
    "  row  = np.array(sparse_tensor.indices[:,0])\n",
    "  col  = np.array(sparse_tensor.indices[:,1])\n",
    "  data = np.array(sparse_tensor.values)\n",
    "  out = scipy.sparse.coo_matrix((data, (row, col)), shape=(sparse_tensor.shape.as_list()))\n",
    "\n",
    "  return out\n",
    "\n",
    "def create_transformer():\n",
    "    sequence_length = 100\n",
    "    max_features = 1000000\n",
    "    # Token locations\n",
    "    Vectorizer_transformer = tf.keras.layers.TextVectorization(max_tokens=max_features,output_sequence_length=sequence_length) \n",
    "    Vectorizer_transformer.adapt(np.hstack((X,df_kaggle['text'])))\n",
    "    vocab = Vectorizer_transformer.get_vocabulary()\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "    embed_dim =32  # Embedding size for each token\n",
    "    num_heads =1  # Number of attention heads\n",
    "    ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "    maxlen = sequence_length\n",
    "    dropout_rate = 0.2 # Dropout rate of feed forward network \n",
    "\n",
    "    ## Build embedding and transformer\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim,dropout_rate)\n",
    "\n",
    "    ## Connect Keras Layers\n",
    "    inputs = tf.keras.Input(shape=(1,), dtype=tf.string) \n",
    "    vec = Vectorizer_transformer(inputs)\n",
    "    x = embedding_layer(vec)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    transformer = keras.Model(inputs=inputs, outputs=outputs) ##Final Model\n",
    "    \n",
    "    transformer.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              metrics=['accuracy'])\n",
    "    return transformer\n",
    "\n",
    "def create_hybrid(conv_filters, conv_size, lstm_units):\n",
    "    model = tf.keras.Sequential([\n",
    "      encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(vocab),\n",
    "        output_dim=128,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Conv1D(filters=conv_filters,\n",
    "                            kernel_size=conv_size,\n",
    "                            padding=\"same\",\n",
    "                            activation=\"relu\",\n",
    "                            data_format=\"channels_last\",\n",
    "                            ),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True)),\n",
    "    tf.keras.layers.GlobalMaxPool1D(keepdims=False),\n",
    "    #tf.keras.layers.Dense(dense_units, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_hybrid2(conv_filters, conv_size, lstm_units, dense_units):\n",
    "    model = tf.keras.Sequential([\n",
    "      encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(vocab),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, dropout=0.2,return_sequences=True)),\n",
    "\n",
    "    tf.keras.layers.Conv1D(filters=conv_filters,\n",
    "                            kernel_size=conv_size,\n",
    "                            padding=\"same\",\n",
    "                            activation=\"relu\",\n",
    "                            data_format=\"channels_last\",\n",
    "                            ),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.GlobalMaxPool1D(keepdims=False),\n",
    "    #tf.keras.layers.Dense(dense_units, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10668,
     "status": "ok",
     "timestamp": 1638580780911,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "8DNzGVuJ-mW_"
   },
   "outputs": [],
   "source": [
    "max_features = 1000000\n",
    "tfidf_vec = tf.keras.layers.TextVectorization(max_tokens=max_features, output_mode='tf_idf', sparse=True, ngrams=3)\n",
    "\n",
    "with tf.device('/device:CPU:0'):\n",
    "    tfidf_vec.adapt(np.hstack((X,df_kaggle['text'])))\n",
    "\n",
    "\n",
    "tdidf = tf.keras.Sequential([\n",
    "    tfidf_vec])\n",
    "count = tf.keras.Sequential([\n",
    "                             \n",
    "                             \n",
    "    count_vec])\n",
    "df = pd.DataFrame(columns = ['model', 'average', 'logloss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638580780912,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "SugMla0s-tpZ",
    "outputId": "e42ba327-446b-466b-d553-f5bb3ed8ce18"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "len(stop)\n",
    "s_upper = []\n",
    "for s in stop:\n",
    "    s_upper.append(s[0].upper()+s[1:])\n",
    "\n",
    "all_stop = stop + s_upper\n",
    "\n",
    "len(all_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1638580780912,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "wSNSXM9o-uCJ"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['model', 'average', 'logloss'])\n",
    "call = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=1,\n",
    "    mode='auto', restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3242195,
     "status": "ok",
     "timestamp": 1638585008431,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "UuvfuDMW-zdp",
    "outputId": "19fe2786-78a9-4bc1-8481-ebf1420b5246"
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "n  = 0\n",
    "\n",
    "X_all_train = np.array([])\n",
    "X_all_test = np.array([])\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    for _ in range(1):\n",
    "\n",
    "        X_train = X.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        X_train_stop = X_train.apply(lambda x: ' '.join([word for word in x.split() if word not in (all_stop)]))\n",
    "        X_test_stop = X_test.apply(lambda x: ' '.join([word for word in x.split() if word not in (all_stop)]))\n",
    "        #X_train = X_train_stop\n",
    "        #X_test = X_test_stop\n",
    "\n",
    "\n",
    "        x_train_sparce = tdidf.predict(X_train)\n",
    "        x_test_sparce = tdidf.predict(X_test)\n",
    "        x_train_sparce_count = count.predict(X_train)\n",
    "        x_test_sparce_count = count.predict(X_test)\n",
    "\n",
    "        x_train_sparce_stop = tdidf.predict(X_train_stop)\n",
    "        x_test_sparce_stop = tdidf.predict(X_test_stop)\n",
    "        x_train_sparce_count_stop = count.predict(X_train_stop)\n",
    "        x_test_sparce_count_stop = count.predict(X_test_stop)\n",
    "\n",
    "        train_count_data = convert_sparce(x_train_sparce_count)\n",
    "        test_count_data = convert_sparce(x_test_sparce_count)\n",
    "        train_count_data_stop = convert_sparce(x_train_sparce_count_stop)\n",
    "        test_count_data_stop = convert_sparce(x_test_sparce_count_stop)\n",
    "\n",
    "        cnn = create_model(64, 128, 32, 32) ##conv1_filters, conv1_size, conv2_filters, conv2_size\n",
    "        ngram = create_ngram()\n",
    "        LSTM = create_lstm()\n",
    "        transformer = create_transformer()\n",
    "        hybrid = create_hybrid(64, 32, 64) ## conv_filters, conv_size, lstm_units\n",
    "        ensemble = create_ensemble()\n",
    "        ensemble_with_tdidf = create_ensemble()\n",
    "        multi_nb = MultinomialNB(alpha=1.5)\n",
    "\n",
    "        multi_nb.fit(train_count_data, np.argmax(y_train, axis =1))\n",
    "\n",
    "        cnn.fit(X_train, y_train, epochs=100, batch_size=256,validation_data= (X_test, y_test), callbacks=[call])\n",
    "\n",
    "        ngram.fit(X_train, y_train, epochs=100, batch_size=64,validation_data= (X_test, y_test), callbacks=[call])\n",
    "\n",
    "        LSTM.fit(X_train, y_train, epochs=100, batch_size=64,validation_data= (X_test, y_test), callbacks=[call])\n",
    "        transformer.fit(X_train, y_train, epochs=100, batch_size=128,validation_data= (X_test, y_test), callbacks=[call])\n",
    "\n",
    "        hybrid.fit(X_train, y_train, epochs=100, batch_size=32,validation_data= (X_test, y_test), callbacks=[call])\n",
    "\n",
    "        cnn_pred = cnn.predict(X_train)\n",
    "        ngram_pred = ngram.predict(X_train)\n",
    "        LSTM_pred = LSTM.predict(X_train)\n",
    "        transformer_pred = transformer.predict(X_train)\n",
    "        hybrid_pred = hybrid.predict(X_train)\n",
    "\n",
    "        cnn_pred_test = cnn.predict(X_test)\n",
    "        ngram_pred_test = ngram.predict(X_test)\n",
    "        LSTM_pred_test = LSTM.predict(X_test)\n",
    "        transformer_pred_test = transformer.predict(X_test)\n",
    "        hybrid_pred_test = hybrid.predict(X_test)\n",
    "\n",
    "\n",
    "        multi_nb_pred = multi_nb.predict_proba(train_count_data)\n",
    "        multi_nb_pred_test = multi_nb.predict_proba(test_count_data)\n",
    "\n",
    "\n",
    "        com_nb = ComplementNB(alpha = 0.9)\n",
    "        com_nb.fit(train_count_data, np.argmax(y_train, axis =1))\n",
    "        com_nb_pred = com_nb.predict_proba(train_count_data)\n",
    "        com_nb_pred_test = com_nb.predict_proba(test_count_data)\n",
    "        com_nb_logloss = log_loss(y_test,com_nb_pred_test)\n",
    "        com_nb_acc = np.sum(np.argmax(y_test, axis = 1) == com_nb.predict(test_count_data))/len(np.argmax(y_test, axis = 1))\n",
    "\n",
    "        \n",
    "        \n",
    "        cnn_stop = create_model(64, 128, 32, 32) ##conv1_filters, conv1_size, conv2_filters, conv2_size\n",
    "        ngram_stop = create_ngram()\n",
    "        LSTM_stop = create_lstm()\n",
    "        transformer_stop = create_transformer()\n",
    "        hybrid_stop = create_hybrid(64, 32, 64) ## conv_filters, conv_size, lstm_units\n",
    "        ensemble_stop = create_ensemble()\n",
    "        ensemble_with_tdidf_stop = create_ensemble()\n",
    "        multi_nb_stop = MultinomialNB(alpha=1.5)\n",
    "\n",
    "        multi_nb_stop.fit(train_count_data_stop, np.argmax(y_train, axis =1))\n",
    "\n",
    "        cnn_stop.fit(X_train_stop, y_train, epochs=100, batch_size=256,validation_data= (X_test_stop, y_test), callbacks=[call])\n",
    "\n",
    "        ngram_stop.fit(X_train_stop, y_train, epochs=100, batch_size=64,validation_data= (X_test_stop, y_test), callbacks=[call])\n",
    "\n",
    "        LSTM_stop.fit(X_train_stop, y_train, epochs=100, batch_size=64,validation_data= (X_test_stop, y_test), callbacks=[call])\n",
    "        transformer_stop.fit(X_train_stop, y_train, epochs=100, batch_size=128,validation_data= (X_test_stop, y_test), callbacks=[call])\n",
    "\n",
    "        hybrid_stop.fit(X_train_stop, y_train, epochs=100, batch_size=32,validation_data= (X_test_stop, y_test), callbacks=[call])\n",
    "\n",
    "        cnn_pred_stop = cnn_stop.predict(X_train_stop)\n",
    "        ngram_pred_stop = ngram_stop.predict(X_train_stop)\n",
    "        LSTM_pred_stop = LSTM_stop.predict(X_train_stop)\n",
    "        transformer_pred_stop = transformer_stop.predict(X_train_stop)\n",
    "        hybrid_pred_stop = hybrid_stop.predict(X_train_stop)\n",
    "\n",
    "        cnn_pred_test_stop = cnn_stop.predict(X_test_stop)\n",
    "        ngram_pred_test_stop = ngram_stop.predict(X_test_stop)\n",
    "        LSTM_pred_test_stop = LSTM_stop.predict(X_test_stop)\n",
    "        transformer_pred_test_stop = transformer.predict(X_test_stop)\n",
    "        hybrid_pred_test_stop = hybrid_stop.predict(X_test_stop)\n",
    "\n",
    "\n",
    "        multi_nb_pred_stop = multi_nb_stop.predict_proba(train_count_data_stop)\n",
    "        multi_nb_pred_test_stop = multi_nb_stop.predict_proba(test_count_data_stop)\n",
    "\n",
    "\n",
    "        com_nb_stop = ComplementNB(alpha = 0.9)\n",
    "        com_nb_stop.fit(train_count_data_stop, np.argmax(y_train, axis =1))\n",
    "        com_nb_pred_stop = com_nb_stop.predict_proba(train_count_data_stop)\n",
    "        com_nb_pred_test_stop = com_nb_stop.predict_proba(test_count_data_stop)\n",
    "        com_nb_logloss_stop = log_loss(y_test,com_nb_pred_test_stop)\n",
    "        com_nb_acc_stop = np.sum(np.argmax(y_test, axis = 1) == com_nb_stop.predict(test_count_data_stop))/len(np.argmax(y_test, axis = 1))\n",
    "\n",
    "\n",
    "\n",
    "        X_train_ens = np.hstack([ngram_pred,cnn_pred,LSTM_pred,transformer_pred, hybrid_pred,multi_nb_pred,com_nb_pred,\n",
    "    ngram_pred_stop,cnn_pred_stop,LSTM_pred_stop,transformer_pred_stop, hybrid_pred_stop,multi_nb_pred_stop,com_nb_pred_stop])\n",
    "        \n",
    "        X_test_ens = np.hstack([ngram_pred_test,cnn_pred_test,LSTM_pred_test,transformer_pred_test,hybrid_pred_test,multi_nb_pred_test,com_nb_pred_test,\n",
    "ngram_pred_test_stop,cnn_pred_test_stop,LSTM_pred_test_stop,transformer_pred_test_stop,hybrid_pred_test_stop,multi_nb_pred_test_stop,com_nb_pred_test_stop])\n",
    "\n",
    "        X_train_final_tensor = tf.sparse.from_dense(X_train_ens)\n",
    "        X_test_final_tensor = tf.sparse.from_dense(X_test_ens)\n",
    "        #X_train_concat_tensor = tf.sparse.concat(1,[tf.dtypes.cast(x_train_sparce, tf.float64),tf.dtypes.cast(x_train_sparce_stop, tf.float64), X_train_final_tensor])\n",
    "        #X_test_concat_tensor = tf.sparse.concat(1,[tf.dtypes.cast(x_test_sparce, tf.float64),tf.dtypes.cast(x_test_sparce_stop, tf.float64), X_test_final_tensor])\n",
    "        #X_train_concat_tensor = tf.sparse.concat(1,[tf.dtypes.cast(x_train_sparce, tf.float64), X_train_final_tensor])\n",
    "        #X_test_concat_tensor = tf.sparse.concat(1,[tf.dtypes.cast(x_test_sparce, tf.float64), X_test_final_tensor])\n",
    "\n",
    "        #ensemble.fit(X_train_ens, y_train, epochs=2, batch_size=128)\n",
    "        #ensemble_with_tdidf.fit(X_train_concat_tensor, y_train, epochs=2, batch_size=128)\n",
    "\n",
    "\n",
    "\n",
    "        ngram_results = ngram.evaluate(X_test,y_test)\n",
    "        LSTM_results =LSTM.evaluate(X_test,y_test)\n",
    "        cnn_results =cnn.evaluate(X_test,y_test)\n",
    "        transformer_results = transformer.evaluate(X_test,y_test)\n",
    "        hybrid_results = hybrid.evaluate(X_test,y_test)\n",
    "        #ensemble_results =ensemble.evaluate(X_test_ens,y_test)\n",
    "        #ensemble_with_tdidf_results =ensemble_with_tdidf.evaluate(X_test_concat_tensor,y_test)\n",
    "\n",
    "        multi_nb_logloss = log_loss(y_test,multi_nb_pred_test)\n",
    "        multi_nb_acc = np.sum(np.argmax(y_test, axis = 1) == multi_nb.predict(test_count_data))/len(np.argmax(y_test, axis = 1))\n",
    "        if _ == 0:\n",
    "            X_all_train = X_train_ens\n",
    "            X_all_test = X_test_ens\n",
    "            continue\n",
    "        X_all_train = np.hstack((X_all_train,X_train_ens))\n",
    "        X_all_test = np.hstack((X_all_test,X_test_ens))\n",
    "\n",
    "    if n == 0:\n",
    "        break\n",
    "    n+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1638585008795,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "TnxZ3PtrBl8S",
    "outputId": "b00c5608-ead3-4bfb-edf5-fb73133fc363"
   },
   "outputs": [],
   "source": [
    "print(ngram_results)\n",
    "print(LSTM_results)\n",
    "print(cnn_results )\n",
    "print(transformer_results)\n",
    "print(hybrid_results) \n",
    "#print(ensemble_results)\n",
    "print(multi_nb_logloss)\n",
    "print(com_nb_logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3084,
     "status": "ok",
     "timestamp": 1638585063585,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "dTWbWfgqCcob",
    "outputId": "18f120ea-865b-412c-b7dc-5631bfa37b11"
   },
   "outputs": [],
   "source": [
    "print(X_train_all.shape)\n",
    "X_train_final_tensor = tf.sparse.from_dense(X_train_ens)\n",
    "X_test_final_tensor = tf.sparse.from_dense(X_test_ens)\n",
    "X_train_concat_tensor = tf.sparse.concat(1,[tf.dtypes.cast(x_train_sparce, tf.float64), X_train_final_tensor])\n",
    "X_test_concat_tensor = tf.sparse.concat(1,[tf.dtypes.cast(x_test_sparce, tf.float64), X_test_final_tensor])\n",
    "\n",
    "X_train_sp_all = tf.sparse.from_dense(X_all_train)\n",
    "X_test_sp_all = tf.sparse.from_dense(X_all_test)\n",
    "X_train_concat_all = tf.sparse.concat(1,[tf.dtypes.cast(x_train_sparce, tf.float64), X_train_sp_all])\n",
    "X_test_concat_all = tf.sparse.concat(1,[tf.dtypes.cast(x_test_sparce, tf.float64), X_test_sp_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1638585063586,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "_rubTgZrCk9K",
    "outputId": "80efebd6-ba4c-4b64-d94b-2b5e8b41f1e3"
   },
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression(fit_intercept=False, positive= True)\n",
    "lin_reg.fit(X_train_ens, y_train)\n",
    "linreg_logloss = log_loss(y_test,lin_reg.predict(X_test_ens))\n",
    "linreg_acc = np.sum(np.argmax(y_test, axis = 1) == np.argmax(lin_reg.predict(X_test_ens), axis = 1))/len(np.argmax(y_test, axis = 1))\n",
    "\n",
    "lin_reg_all = LinearRegression(fit_intercept=False, positive= True)\n",
    "lin_reg_all.fit(X_all_train, y_train)\n",
    "linreg_logloss_all = log_loss(y_test,lin_reg_all.predict(X_all_test))\n",
    "linreg_acc_all = np.sum(np.argmax(y_test, axis = 1) == np.argmax(lin_reg_all.predict(X_all_test), axis = 1))/len(np.argmax(y_test, axis = 1))\n",
    "\n",
    "print(linreg_logloss)\n",
    "print(linreg_logloss_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21890,
     "status": "ok",
     "timestamp": 1638585090086,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "ypwiur8pE-Qc"
   },
   "outputs": [],
   "source": [
    "X_train_pre = convert_sparce(x_train_sparce)\n",
    "X_test_pre = convert_sparce(x_test_sparce)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, n_iter=10)\n",
    "svd.fit(X_train_pre)\n",
    "svd_train = np.hstack((svd.transform(X_train_pre),X_all_train))\n",
    "svd_test = np.hstack((svd.transform(X_test_pre),X_all_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20432,
     "status": "ok",
     "timestamp": 1638585423709,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "DvWV-p7UFFls",
    "outputId": "9c2e628a-ffc7-4cfe-e580-7f72f37f0bf3"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "d_tree = ExtraTreesClassifier(n_estimators=5000,criterion='entropy', max_depth=4)\n",
    "#d_tree.fit(X_train_ens, np.argmax(y_train, axis =1))\n",
    "\n",
    "print(svd_train.shape)\n",
    "d_tree.fit(svd_train, y_train)\n",
    "\n",
    "d_tree_pred = d_tree.predict_proba(svd_test)\n",
    "\n",
    "#d_tree_logloss = log_loss(y_test,d_tree_pred)\n",
    "d_tree_logloss = log_loss(y_test,np.transpose(np.array(d_tree.predict_proba(svd_test))[:,:,-1]))\n",
    "\n",
    "#d_tree_acc = np.sum(np.argmax(y_test, axis = 1) == d_tree.predict(X_test_ens))/len(np.argmax(y_test, axis = 1))\n",
    "d_tree_acc = np.sum(np.argmax(y_test, axis = 1) == np.argmax(np.transpose(np.array(d_tree.predict_proba(svd_test))[:,:,-1]),axis =1 ))/len(np.argmax(y_test, axis = 1))\n",
    "\n",
    "print(d_tree_logloss,d_tree_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "d_tree = ExtraTreesClassifier(n_estimators=5000,criterion='entropy', max_depth=4)\n",
    "#d_tree.fit(X_train_ens, np.argmax(y_train, axis =1))\n",
    "\n",
    "print(X_all_train.shape)\n",
    "d_tree.fit(X_all_train, y_train)\n",
    "\n",
    "d_tree_pred = d_tree.predict_proba(X_all_test)\n",
    "\n",
    "#d_tree_logloss = log_loss(y_test,d_tree_pred)\n",
    "d_tree_logloss = log_loss(y_test,np.transpose(np.array(d_tree.predict_proba(X_all_test))[:,:,-1]))\n",
    "\n",
    "#d_tree_acc = np.sum(np.argmax(y_test, axis = 1) == d_tree.predict(X_test_ens))/len(np.argmax(y_test, axis = 1))\n",
    "d_tree_acc = np.sum(np.argmax(y_test, axis = 1) == np.argmax(np.transpose(np.array(d_tree.predict_proba(X_all_test))[:,:,-1]),axis =1 ))/len(np.argmax(y_test, axis = 1))\n",
    "\n",
    "print(d_tree_logloss,d_tree_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "executionInfo": {
     "elapsed": 167726,
     "status": "error",
     "timestamp": 1638585339801,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "w5aL_06MFGmE",
    "outputId": "1084d60a-c9f3-4aea-da15-88049bac8c3b"
   },
   "outputs": [],
   "source": [
    "e2 = create_ensemble()\n",
    "e3 = create_ensemble()\n",
    "e4 = create_ensemble()\n",
    "e5 = create_ensemble()\n",
    "\n",
    "  \n",
    "e2.fit(X_train_ens, y_train, epochs=2, batch_size=128, validation_data= (X_test_ens, y_test))\n",
    "e3.fit(X_all_train, y_train, epochs=2, batch_size=128, validation_data= (X_all_test, y_test))\n",
    "e4.fit(X_train_concat_tensor, y_train, epochs=2, batch_size=128,validation_data= (X_test_concat_tensor, y_test))\n",
    "e5.fit(X_train_concat_all, y_train, epochs=3, batch_size=256,validation_data= (X_test_concat_all, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1638585343358,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "uFp5Rqv2SX_j",
    "outputId": "6a2dfd1e-5384-42fe-8698-64b1c9a3441d"
   },
   "outputs": [],
   "source": [
    "ngram_wrong = np.argmax(ngram_pred_test, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_ngram = np.argwhere(ngram_wrong).reshape(len(np.argwhere(ngram_wrong)),)\n",
    "cnn_wrong = np.argmax(cnn_pred_test, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_cnn = np.argwhere(cnn_wrong).reshape(len(np.argwhere(cnn_wrong)),)\n",
    "lstm_wrong = np.argmax(LSTM_pred_test, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_lstm = np.argwhere(lstm_wrong).reshape(len(np.argwhere(lstm_wrong)),)\n",
    "transformer_wrong = np.argmax(transformer_pred_test, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_trans = np.argwhere(transformer_wrong).reshape(len(np.argwhere(transformer_wrong)),)\n",
    "hybrid_wrong = np.argmax(hybrid_pred_test, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_hybrid = np.argwhere(hybrid_wrong).reshape(len(np.argwhere(hybrid_wrong)),)\n",
    "multi_nb_wrong = np.argmax(multi_nb_pred_test, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_multi_nb = np.argwhere(multi_nb_wrong).reshape(len(np.argwhere(multi_nb_wrong)),)\n",
    "com_nb_wrong = np.argmax(com_nb_pred_test, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_com_nb = np.argwhere(com_nb_wrong).reshape(len(np.argwhere(com_nb_wrong)),)\n",
    "\n",
    "\n",
    "all_wrong = set(ind_ngram).intersection(set(ind_cnn),set(ind_lstm),set(ind_hybrid),set(ind_trans),set(ind_multi_nb),set(ind_com_nb))\n",
    "print(len(all_wrong)/len(y_test))\n",
    "\n",
    "\n",
    "ngram_wrong_stop = np.argmax(ngram_pred_test_stop, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_ngram_stop = np.argwhere(ngram_wrong_stop).reshape(len(np.argwhere(ngram_wrong_stop)),)\n",
    "cnn_wrong_stop = np.argmax(cnn_pred_test_stop, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_cnn_stop = np.argwhere(cnn_wrong_stop).reshape(len(np.argwhere(cnn_wrong_stop)),)\n",
    "lstm_wrong_stop = np.argmax(LSTM_pred_test_stop, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_lstm_stop = np.argwhere(lstm_wrong_stop).reshape(len(np.argwhere(lstm_wrong_stop)),)\n",
    "transformer_wrong_stop = np.argmax(transformer_pred_test_stop, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_trans_stop = np.argwhere(transformer_wrong_stop).reshape(len(np.argwhere(transformer_wrong_stop)),)\n",
    "hybrid_wrong_stop = np.argmax(hybrid_pred_test_stop, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_hybrid_stop = np.argwhere(hybrid_wrong_stop).reshape(len(np.argwhere(hybrid_wrong_stop)),)\n",
    "multi_nb_wrong_stop = np.argmax(multi_nb_pred_test_stop, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_multi_nb_stop = np.argwhere(multi_nb_wrong_stop).reshape(len(np.argwhere(multi_nb_wrong_stop)),)\n",
    "com_nb_wrong_stop = np.argmax(com_nb_pred_test_stop, axis = 1) != np.argmax(y_test, axis = 1)\n",
    "ind_com_nb_stop = np.argwhere(com_nb_wrong_stop).reshape(len(np.argwhere(com_nb_wrong_stop)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1638585495945,
     "user": {
      "displayName": "Michael Dominguez",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14258183258878183412"
     },
     "user_tz": 300
    },
    "id": "-cZ0eLxKS65L",
    "outputId": "2491fadd-3bf7-44fe-9e69-d8fd51d97994"
   },
   "outputs": [],
   "source": [
    "all_wrong = set(ind_ngram).intersection(set(ind_cnn),set(ind_lstm),set(ind_hybrid),set(ind_trans),set(ind_multi_nb),set(ind_com_nb))\n",
    "print(len(all_wrong)/len(y_test))\n",
    "\n",
    "all_wrong_stop = set(ind_ngram_stop).intersection(set(ind_cnn_stop),set(ind_lstm_stop),set(ind_hybrid_stop),set(ind_trans_stop),set(ind_multi_nb_stop),set(ind_com_nb_stop))\n",
    "print(len(all_wrong_stop)/len(y_test))\n",
    "print(len(set(all_wrong).intersection(set(all_wrong_stop)))/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPGM9uvITnCb"
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "#test_train = np.hstack((X_train_ens,ensemble_with_tdidf.predict(X_train_concat_tensor)))\n",
    "#test_test= np.hstack((X_test_ens,ensemble_with_tdidf.predict(X_test_concat_tensor)))\n",
    "\n",
    "\n",
    "eval_set = [(svd_train, np.argmax(y_train,axis = 1)), (svd_test, np.argmax(y_test,axis = 1))]\n",
    "\n",
    "xgb = XGBClassifier( colsample_bytree = .01,\n",
    "                          subsample = .8,\n",
    "                          learning_rate = 0.1,\n",
    "                          max_depth = 4,\n",
    "                          num_class =3,\n",
    "                          objective ='multi:softprob',\n",
    "\n",
    "                          n_estimators =5000,)\n",
    "\n",
    "xgb.fit(svd_train, np.argmax(y_train,axis = 1),  early_stopping_rounds=50, eval_metric=[ \"mlogloss\"], eval_set=eval_set,verbose=2000)\n",
    "\n",
    "xgb_pred = xgb.predict_proba(svd_test)\n",
    "xgb_logloss = log_loss(y_test,xgb_pred)\n",
    "xgb_acc = np.sum(np.argmax(y_test, axis = 1) == xgb.predict(svd_test))/len(np.argmax(y_test, axis = 1))\n",
    "\n",
    "print(xgb_acc,xgb_logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNMY4YILjfAn3xXETP+rbv/",
   "collapsed_sections": [],
   "name": "test_all.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
