{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kfold_added_models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVcv8kC5zeZl"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict,KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import scipy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1mHTfKYzkCQ",
        "outputId": "23b67a89-aa19-4602-9185-f35eba8cfaaf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LjZV37Tkzlw3",
        "outputId": "8b70a2fb-212a-4727-db75-b22a1d0e0f18"
      },
      "source": [
        "import os            ##  This module is for \"operating system\" interfaces\n",
        "import sys           ##  This module is for functionality relevant to the python run time\n",
        "\n",
        "GOOGLE_PATH_AFTER_MYDRIVE = 'Data'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive','My Drive', GOOGLE_PATH_AFTER_MYDRIVE)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
        "\n",
        "# Append the directory path of this notebook to what python easily \"sees\"\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "\n",
        "# Make your current working direct\n",
        "GOOGLE_DRIVE_PATH"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample_submission.csv', 'test.csv', 'train.csv', 'submission.csv']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'drive/My Drive/Data'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WSyHZuKxztdn",
        "outputId": "3db73780-5bf0-408f-e6ca-d27d44e8dcc1"
      },
      "source": [
        "df = pd.read_csv('drive/My Drive/Data/train.csv')\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id26305</td>\n",
              "      <td>This process, however, afforded me no means of...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id17569</td>\n",
              "      <td>It never once occurred to me that the fumbling...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id11008</td>\n",
              "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27763</td>\n",
              "      <td>How lovely is spring As we looked from Windsor...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id12958</td>\n",
              "      <td>Finding nothing else, not even gold, the Super...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text author\n",
              "0  id26305  This process, however, afforded me no means of...    EAP\n",
              "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
              "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
              "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
              "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI6T1l-4zv03"
      },
      "source": [
        "\n",
        "X = df[\"text\"].copy()\n",
        "#X = df[\"text\"]\n",
        "\n",
        "authors = df[\"author\"].copy()\n",
        "\n",
        "# Label data\n",
        "y = []\n",
        "for author in authors:\n",
        "    if author == \"EAP\":\n",
        "        y.append([1, 0, 0])\n",
        "    if author == \"HPL\":\n",
        "        y.append([0, 1, 0])\n",
        "    if author == \"MWS\":\n",
        "        y.append([0, 0, 1])\n",
        "\n",
        "y = np.array(y)\n",
        "\n",
        "y_one_vector = []\n",
        "for author in authors:\n",
        "    if author == \"EAP\":\n",
        "        y_one_vector.append(0)\n",
        "    if author == \"HPL\":\n",
        "        y_one_vector.append(1)\n",
        "    if author == \"MWS\":\n",
        "        y_one_vector.append(2)\n",
        "\n",
        "y_one_vector = np.array(y_one_vector)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpRuE6uizx5f"
      },
      "source": [
        "encoder = tf.keras.layers.TextVectorization()\n",
        "encoder.adapt(X)\n",
        "\n",
        "max_features = 1000000\n",
        "Vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_features, output_mode='tf_idf', ngrams=2)\n",
        "with tf.device('/device:CPU:0'):\n",
        "  Vectorizer.adapt(X)\n",
        "\n",
        "vocab = encoder.get_vocabulary()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgdHoMcSs1eo"
      },
      "source": [
        "def convert_sparce(sparse_tensor):\n",
        "  \n",
        "  row  = np.array(sparse_tensor.indices[:,0])\n",
        "  col  = np.array(sparse_tensor.indices[:,1])\n",
        "  data = np.array(sparse_tensor.values)\n",
        "  out = scipy.sparse.coo_matrix((data, (row, col)), shape=(sparse_tensor.shape.as_list()))\n",
        "\n",
        "  return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4LeJHVMYjw1"
      },
      "source": [
        "# TRANSFORMER\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim, mask_zero=True)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbjP8zPxz3p3"
      },
      "source": [
        "class CNN1d(tf.keras.Model):\n",
        "    def __init__(self, conv1_filters, conv1_size, conv2_filters, conv2_size, dense1, encoder):\n",
        "        super(CNN1d, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "\n",
        "        vocab = encoder.get_vocabulary()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=len(vocab),output_dim=64,mask_zero=True)\n",
        "        \n",
        "\n",
        "        self.conv1 = tf.keras.layers.Conv1D(filters=conv1_filters,\n",
        "                            kernel_size=conv1_size,\n",
        "                            padding=\"same\",\n",
        "                            activation=\"relu\",\n",
        "                            data_format=\"channels_last\",\n",
        "                            )\n",
        "        self.conv2 = tf.keras.layers.Conv1D(filters=conv2_filters,\n",
        "                            kernel_size=conv2_size,\n",
        "                            padding=\"same\",\n",
        "                            activation=\"relu\",\n",
        "                            data_format=\"channels_last\",\n",
        "                            )\n",
        "        self.global_pool = tf.keras.layers.GlobalMaxPool1D(keepdims=False)\n",
        "        self.dense1 = tf.keras.layers.Dense(dense1, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(3, activation=\"softmax\")\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        emb = self.encoder(x)\n",
        "        emb = self.embedding(emb)\n",
        "        conv1 = self.conv1(emb)\n",
        "        conv2 = self.conv2(emb)\n",
        "        z = tf.concat([conv1, conv2], axis=2)\n",
        "        z = self.global_pool(z)\n",
        "        z = self.dense1(z)\n",
        "        z = self.dense2(z)\n",
        "        return z"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bob8Sjtbz5Mv"
      },
      "source": [
        "def create_cnn(conv1_filters, conv1_size, conv2_filters, conv2_size, dense1):\n",
        "    model = CNN1d(conv1_filters, conv1_size, conv2_filters, conv2_size, dense1, encoder)\n",
        "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "              metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def create_ngram():\n",
        "    model_ngram = tf.keras.Sequential()\n",
        "    model_ngram.add(Vectorizer)\n",
        "      \n",
        "    model_ngram.add(tf.keras.layers.Dense(25, activation='relu'))\n",
        "    model_ngram.add(tf.keras.layers.Dropout(0.2))\n",
        "      \n",
        "    model_ngram.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "      \n",
        "    model_ngram.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                metrics=['accuracy'])\n",
        "    return model_ngram\n",
        "\n",
        "def create_lstm():\n",
        "    LSTM = tf.keras.Sequential()\n",
        "    LSTM.add(encoder)\n",
        "    LSTM.add(tf.keras.layers.Embedding(input_dim=len(vocab),output_dim=64,mask_zero=True))\n",
        "      \n",
        "    LSTM.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,dropout=0.2,return_sequences=True)))\n",
        "    LSTM.add(tf.keras.layers.GlobalMaxPool1D())\n",
        "\n",
        "    LSTM.add(tf.keras.layers.Dropout(0.2))\n",
        "      \n",
        "    LSTM.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "      \n",
        "    LSTM.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    return LSTM\n",
        "\n",
        "def create_ensemble():\n",
        "    ensemble = tf.keras.Sequential()\n",
        "    # for 3 model\n",
        "    ensemble.add(tf.keras.layers.Dense(36, activation='relu'))\n",
        "    ensemble.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "    ensemble.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "    #ensemble.add(tf.keras.layers.InputLayer())\n",
        "\n",
        "    ensemble.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    return ensemble\n",
        "\n",
        "def create_transformer():\n",
        "  sequence_length = 100\n",
        "  max_features = 1000000\n",
        "  # Token locations\n",
        "  Vectorizer_transformer = tf.keras.layers.TextVectorization(max_tokens=max_features,output_sequence_length=sequence_length) \n",
        "  Vectorizer_transformer.adapt(X)\n",
        "  vocab = Vectorizer_transformer.get_vocabulary()\n",
        "  vocab_size = len(vocab)\n",
        "\n",
        "\n",
        "  embed_dim =32  # Embedding size for each token\n",
        "  num_heads =2  # Number of attention heads\n",
        "  ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "  maxlen = sequence_length\n",
        "  dropout_rate = 0.3 # Dropout rate of feed forward network \n",
        "\n",
        "  ## Build embedding and transformer\n",
        "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "  transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim,dropout_rate)\n",
        "\n",
        "  ## Connect Keras Layers\n",
        "  inputs = tf.keras.Input(shape=(1,), dtype=tf.string) \n",
        "  vec = Vectorizer_transformer(inputs)\n",
        "  x = embedding_layer(vec)\n",
        "  x = transformer_block(x)\n",
        "  x = layers.GlobalAveragePooling1D()(x)\n",
        "  outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
        "\n",
        "  transformer = keras.Model(inputs=inputs, outputs=outputs) ##Final Model\n",
        "\n",
        "  transformer.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                metrics=['accuracy'])\n",
        "  return transformer\n",
        "\n",
        "def create_hybrid(conv_filters, conv_size, lstm_units, dense_units):\n",
        "  model = tf.keras.Sequential([\n",
        "      encoder,\n",
        "      tf.keras.layers.Embedding(\n",
        "          input_dim=len(vocab),\n",
        "          output_dim=64,\n",
        "          # Use masking to handle the variable sequence lengths\n",
        "          mask_zero=True),\n",
        "      tf.keras.layers.Conv1D(filters=conv_filters,\n",
        "                              kernel_size=conv_size,\n",
        "                              padding=\"same\",\n",
        "                              activation=\"relu\",\n",
        "                              data_format=\"channels_last\",\n",
        "                              ),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=False)),\n",
        "      # tf.keras.layers.GlobalMaxPool1D(keepdims=False),\n",
        "      tf.keras.layers.Dense(dense_units, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(3, activation=\"softmax\")\n",
        "  ])\n",
        "  model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "            optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "            metrics=['accuracy']\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def create_multinb():\n",
        "  return MultinomialNB(alpha=1.5)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_sWe-F90tK8"
      },
      "source": [
        "max_features = 1000000\n",
        "tfidf_vec = tf.keras.layers.TextVectorization(max_tokens=max_features, output_mode='tf_idf', sparse=True, ngrams=2)\n",
        "\n",
        "with tf.device('/device:CPU:0'):\n",
        "  tfidf_vec.adapt(X)\n",
        "\n",
        "tdidf = tf.keras.Sequential([\n",
        "    tfidf_vec])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "kcnlddBUz7F_",
        "outputId": "a040bbd5-f5c9-40a4-86a5-c3fbc9a0f97f"
      },
      "source": [
        "df = pd.DataFrame(columns = ['model', 'average', 'logloss'])\n",
        "df"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>average</th>\n",
              "      <th>logloss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [model, average, logloss]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "-XcAYPZIz-R_",
        "outputId": "cdcc1d7b-b5b5-4995-c0d6-d9f7beefcdab"
      },
      "source": [
        "count_vec = tf.keras.layers.TextVectorization(max_tokens=max_features, output_mode='count', sparse=True, ngrams=1)\n",
        "count_vec.adapt(X)\n",
        "count = tf.keras.Sequential([count_vec])\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=3)\n",
        "\n",
        "for train_index, test_index in kf.split(X[:10]):\n",
        "\n",
        "  X_train = X.iloc[train_index]\n",
        "  X_test = X.iloc[test_index]\n",
        "  y_train = y[train_index]\n",
        "  y_test = y[test_index]\n",
        "\n",
        "\n",
        "  x_train_sparce = tdidf.predict(X_train)\n",
        "  x_test_sparce = tdidf.predict(X_test)\n",
        "  x_train_sparce_count = count.predict(X_train)\n",
        "  x_test_sparce_count = count.predict(X_test)\n",
        "\n",
        "  train_tdift_data = convert_sparce(x_train_sparce)\n",
        "  test_tdift_data = convert_sparce(x_test_sparce)\n",
        "  train_count_data = convert_sparce(x_train_sparce_count)\n",
        "  test_count_data = convert_sparce(x_test_sparce_count)\n",
        "\n",
        "\n",
        "\n",
        "  cnn = create_cnn(128, 6, 128, 5, 128)\n",
        "  ngram = create_ngram()\n",
        "  LSTM = create_lstm()\n",
        "  transformer = create_transformer()\n",
        "  hybrid = create_hybrid(64, 5, 64, 64)\n",
        "  multi_nb = create_multinb()\n",
        "  ensemble = create_ensemble()\n",
        "  ensemble_with_tdidf = create_ensemble()\n",
        "\n",
        "  cnn.fit(X_train, y_train, epochs=1 )\n",
        "\n",
        "  ngram.fit(X_train, y_train, epochs=1, batch_size=64)\n",
        "  \n",
        "  LSTM.fit(X_train, y_train, epochs=2, batch_size=64)\n",
        "\n",
        "  transformer.fit(X_train, y_train, epochs=1)\n",
        "\n",
        "  hybrid.fit(X_train, y_train, epochs=1)\n",
        "\n",
        "  multi_nb.fit(train_count_data, np.argmax(y_train, axis =1))\n",
        "  \n",
        "\n",
        "  cnn_pred = cnn.predict(X_train)\n",
        "  ngram_pred = ngram.predict(X_train)\n",
        "  LSTM_pred = LSTM.predict(X_train)\n",
        "  transformer_pred = transformer.predict(X_train)\n",
        "  hybrid_pred = hybrid.predict(X_train)\n",
        "  multi_nb_pred = multi_nb.predict_proba(train_count_data)\n",
        "\n",
        "\n",
        "  cnn_pred_test = cnn.predict(X_test)\n",
        "  ngram_pred_test = ngram.predict(X_test)\n",
        "  LSTM_pred_test = LSTM.predict(X_test)\n",
        "  transformer_pred_test = transformer.predict(X_test)\n",
        "  hybrid_pred_test = hybrid.predict(X_test)\n",
        "  multi_nb_pred_test = multi_nb.predict_proba(test_count_data)\n",
        "\n",
        "\n",
        "\n",
        "  X_train_ens = np.hstack([ngram_pred,cnn_pred,LSTM_pred, transformer_pred, hybrid_pred,multi_nb_pred])\n",
        "  X_test_ens = np.hstack([ngram_pred_test,cnn_pred_test,LSTM_pred_test, transformer_pred_test, hybrid_pred_test,multi_nb_pred_test])\n",
        "\n",
        "\n",
        "  X_train_final_tensor = tf.sparse.from_dense(X_train_ens)\n",
        "  X_test_final_tensor = tf.sparse.from_dense(X_test_ens)\n",
        "  X_train_concat_tensor = tf.sparse.concat(1,[x_train_sparce, X_train_final_tensor])\n",
        "  X_test_concat_tensor = tf.sparse.concat(1,[x_test_sparce, X_test_final_tensor])\n",
        "\n",
        "  multi_nb_logloss = log_loss(y_test,multi_nb_pred)\n",
        "  multi_nb_acc = np.sum(np.argmax(y_test, axis = 1) == multi_nb.predict(test_count_data))/len(np.argmax(y_test, axis = 1))\n",
        "\n",
        "\n",
        "  ensemble.fit(X_train_ens, y_train, epochs=2, batch_size=128)\n",
        "  ensemble_with_tdidf.fit(X_train_concat_tensor, y_train, epochs=1, batch_size=256)\n",
        "\n",
        "  ngram_results = ngram.evaluate(X_test,y_test)\n",
        "  LSTM_results =LSTM.evaluate(X_test,y_test)\n",
        "  cnn_results =cnn.evaluate(X_test,y_test)\n",
        "  transformer_results = transformer.evaluate(X_test, y_test)\n",
        "  hybrid_results = hybrid.evaluate(X_test, y_test)\n",
        "  ensemble_results =ensemble.evaluate(X_test_ens,y_test)\n",
        "  ensemble_with_tdidf_results =ensemble_with_tdidf.evaluate(X_test_concat_tensor,y_test)\n",
        "\n",
        "\n",
        "\n",
        "  lin_reg = LinearRegression(fit_intercept=False, positive= True)\n",
        "  lin_reg.fit(X_train_ens, y_train)\n",
        "  linreg_logloss = log_loss(y_test,lin_reg.predict(X_test_ens))\n",
        "  linreg_acc = np.sum(np.argmax(y_test, axis = 1) == np.argmax(lin_reg.predict(X_test_ens), axis = 1))/len(np.argmax(y_test, axis = 1))\n",
        "\n",
        "\n",
        "  lin_reg2 = LinearRegression(fit_intercept=False, positive= True)\n",
        "  lin_reg2.fit(np.hstack((X_train_ens,ensemble_with_tdidf.predict(X_train_concat_tensor))), y_train)\n",
        "  linreg2_logloss = log_loss(y_test,lin_reg2.predict(np.hstack((X_test_ens,ensemble_with_tdidf.predict(X_test_concat_tensor)\n",
        "  ))))\n",
        "  linreg2_acc = np.sum(np.argmax(y_test, axis = 1) == np.argmax(lin_reg2.predict(np.hstack((X_test_ens,ensemble_with_tdidf.predict(X_test_concat_tensor)\n",
        "  ))), axis = 1))/len(np.argmax(y_test, axis = 1))\n",
        "\n",
        "\n",
        "  df_results = pd.DataFrame({\"model\":['ngram', 'cnn', 'LSTM','transformer','hybrid','multi_nb','ensemble','ensemble_tdidf','lin_reg_1','lin_reg_2'],\\\n",
        "                  \"average\":[ngram_results[1],cnn_results[1],LSTM_results[1],transformer_results[1],hybrid_results[1],multi_nb_acc,ensemble_results[1],ensemble_with_tdidf_results[1],\\\n",
        "                             linreg_acc, linreg2_acc],\\\n",
        "                  \"logloss\":[ngram_results[0],cnn_results[0],LSTM_results[0],transformer_results[0],hybrid_results[0],multi_nb_logloss,ensemble_results[0],ensemble_with_tdidf_results[0],\\\n",
        "                             linreg_logloss, linreg2_logloss]})\n",
        "\n",
        "  df = df.append(df_results)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 979ms/step - loss: 1.0995 - accuracy: 0.1667\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 1.0839 - accuracy: 0.3333\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.1085 - accuracy: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 1.0912 - accuracy: 0.8333\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3361 - accuracy: 0.1667\n",
            "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x7fa83fbd1cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0969 - accuracy: 0.5000\n",
            "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa83e76a5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa83e711c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-68d6bca7e321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mX_train_final_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_ens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0mX_test_final_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_ens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m   \u001b[0mX_train_concat_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train_sparce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_final_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m   \u001b[0mX_test_concat_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test_sparce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_final_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/sparse_ops.py\u001b[0m in \u001b[0;36msparse_concat_v2\u001b[0;34m(axis, sp_inputs, expand_nonconcat_dims, name)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m   output_ind, output_val, output_shape = (\n\u001b[0;32m--> 419\u001b[0;31m       gen_sparse_ops.sparse_concat(inds, vals, shapes, axis, name=name))\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m   \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msp_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_sparse_ops.py\u001b[0m in \u001b[0;36msparse_concat\u001b[0;34m(indices, values, shapes, concat_dim, name)\u001b[0m\n\u001b[1;32m    881\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute SparseConcat as input #3(zero-based) was expected to be a float tensor but is a double tensor [Op:SparseConcat]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7R3Undi0nDH"
      },
      "source": [
        "df.groupby('model').describe()\n",
        "df.iloc[27:,:].groupby('model').describe()\n",
        "\n",
        "df = df.iloc[27:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtGHRot31vIf"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "df.boxplot('average',by = 'model', ax=ax[0])\n",
        "df.boxplot('logloss',by = 'model', ax=ax[1])\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAvc56m81yUH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "0ccf239b-f29a-4b52-c15b-89c5662c48e1"
      },
      "source": [
        "df.groupby('model').describe()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"8\" halign=\"left\">average</th>\n",
              "      <th colspan=\"8\" halign=\"left\">logloss</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>LSTM</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.823025</td>\n",
              "      <td>0.003713</td>\n",
              "      <td>0.819645</td>\n",
              "      <td>0.821037</td>\n",
              "      <td>0.822430</td>\n",
              "      <td>0.824715</td>\n",
              "      <td>0.827000</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.452325</td>\n",
              "      <td>0.012253</td>\n",
              "      <td>0.439696</td>\n",
              "      <td>0.446407</td>\n",
              "      <td>0.453117</td>\n",
              "      <td>0.458640</td>\n",
              "      <td>0.464163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cnn</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.807703</td>\n",
              "      <td>0.007148</td>\n",
              "      <td>0.799448</td>\n",
              "      <td>0.805639</td>\n",
              "      <td>0.811830</td>\n",
              "      <td>0.811830</td>\n",
              "      <td>0.811830</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.484335</td>\n",
              "      <td>0.009327</td>\n",
              "      <td>0.476032</td>\n",
              "      <td>0.479289</td>\n",
              "      <td>0.482545</td>\n",
              "      <td>0.488486</td>\n",
              "      <td>0.494427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ensemble</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.854385</td>\n",
              "      <td>0.004389</td>\n",
              "      <td>0.849395</td>\n",
              "      <td>0.852754</td>\n",
              "      <td>0.856114</td>\n",
              "      <td>0.856880</td>\n",
              "      <td>0.857646</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.392223</td>\n",
              "      <td>0.006302</td>\n",
              "      <td>0.385072</td>\n",
              "      <td>0.389853</td>\n",
              "      <td>0.394634</td>\n",
              "      <td>0.395799</td>\n",
              "      <td>0.396964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ensemble_tdidf</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.859850</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.858872</td>\n",
              "      <td>0.859113</td>\n",
              "      <td>0.859353</td>\n",
              "      <td>0.860339</td>\n",
              "      <td>0.861324</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.365963</td>\n",
              "      <td>0.004685</td>\n",
              "      <td>0.360645</td>\n",
              "      <td>0.364203</td>\n",
              "      <td>0.367761</td>\n",
              "      <td>0.368622</td>\n",
              "      <td>0.369483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hybrid</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.794014</td>\n",
              "      <td>0.007914</td>\n",
              "      <td>0.785780</td>\n",
              "      <td>0.790239</td>\n",
              "      <td>0.794699</td>\n",
              "      <td>0.798131</td>\n",
              "      <td>0.801563</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.514053</td>\n",
              "      <td>0.016528</td>\n",
              "      <td>0.504109</td>\n",
              "      <td>0.504513</td>\n",
              "      <td>0.504918</td>\n",
              "      <td>0.519025</td>\n",
              "      <td>0.533131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lin_reg_1</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.845242</td>\n",
              "      <td>0.002867</td>\n",
              "      <td>0.842936</td>\n",
              "      <td>0.843637</td>\n",
              "      <td>0.844339</td>\n",
              "      <td>0.846396</td>\n",
              "      <td>0.848452</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.400576</td>\n",
              "      <td>0.002782</td>\n",
              "      <td>0.397578</td>\n",
              "      <td>0.399327</td>\n",
              "      <td>0.401075</td>\n",
              "      <td>0.402075</td>\n",
              "      <td>0.403075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lin_reg_2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.860514</td>\n",
              "      <td>0.001319</td>\n",
              "      <td>0.859332</td>\n",
              "      <td>0.859802</td>\n",
              "      <td>0.860273</td>\n",
              "      <td>0.861105</td>\n",
              "      <td>0.861937</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.363380</td>\n",
              "      <td>0.004285</td>\n",
              "      <td>0.358788</td>\n",
              "      <td>0.361433</td>\n",
              "      <td>0.364078</td>\n",
              "      <td>0.365675</td>\n",
              "      <td>0.367273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ngram</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.844936</td>\n",
              "      <td>0.003125</td>\n",
              "      <td>0.842476</td>\n",
              "      <td>0.843178</td>\n",
              "      <td>0.843879</td>\n",
              "      <td>0.846166</td>\n",
              "      <td>0.848452</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.400950</td>\n",
              "      <td>0.002975</td>\n",
              "      <td>0.397576</td>\n",
              "      <td>0.399827</td>\n",
              "      <td>0.402078</td>\n",
              "      <td>0.402637</td>\n",
              "      <td>0.403195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>transformer</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.810920</td>\n",
              "      <td>0.006725</td>\n",
              "      <td>0.803555</td>\n",
              "      <td>0.808013</td>\n",
              "      <td>0.812471</td>\n",
              "      <td>0.814602</td>\n",
              "      <td>0.816733</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.477939</td>\n",
              "      <td>0.015441</td>\n",
              "      <td>0.463871</td>\n",
              "      <td>0.469678</td>\n",
              "      <td>0.475486</td>\n",
              "      <td>0.484973</td>\n",
              "      <td>0.494460</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               average                      ...   logloss                    \n",
              "                 count      mean       std  ...       50%       75%       max\n",
              "model                                       ...                              \n",
              "LSTM               3.0  0.823025  0.003713  ...  0.453117  0.458640  0.464163\n",
              "cnn                3.0  0.807703  0.007148  ...  0.482545  0.488486  0.494427\n",
              "ensemble           3.0  0.854385  0.004389  ...  0.394634  0.395799  0.396964\n",
              "ensemble_tdidf     3.0  0.859850  0.001299  ...  0.367761  0.368622  0.369483\n",
              "hybrid             3.0  0.794014  0.007914  ...  0.504918  0.519025  0.533131\n",
              "lin_reg_1          3.0  0.845242  0.002867  ...  0.401075  0.402075  0.403075\n",
              "lin_reg_2          3.0  0.860514  0.001319  ...  0.364078  0.365675  0.367273\n",
              "ngram              3.0  0.844936  0.003125  ...  0.402078  0.402637  0.403195\n",
              "transformer        3.0  0.810920  0.006725  ...  0.475486  0.484973  0.494460\n",
              "\n",
              "[9 rows x 16 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAVGfknVomVr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}